{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnE1_J-4dhSR"
   },
   "source": [
    "## Загрузим нужные библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uaKzqUxsdjDa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2scLoULoXkT0"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_dataset_train (1).csv\")\n",
    "\n",
    "#df = df.sample(10000)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_dataset_train (1).csv\")\n",
    "\n",
    "mapper_s_el = df.groupby(['station_id']).agg({'entrance_id':lambda x: Counter(x).most_common()[0][0],\n",
    "                               'line_id':lambda x: Counter(x).most_common()[0][0],}).reset_index()\n",
    "\n",
    "df['back_time'] = pd.to_timedelta(df['time_to_under'],unit='m') + pd.to_datetime(df['pass_dttm'])\n",
    "joined = df.copy()\n",
    "joined['pass_dttm'] = pd.to_timedelta(joined['time_to_under'],unit='m') + pd.to_datetime(joined['pass_dttm'])\n",
    "joined['id'] = df['id'].max()+10 +joined['id']\n",
    "\n",
    "joined['temp'] = joined['station_id'].copy()\n",
    "joined['station_id'] = joined['label'].copy()\n",
    "joined['label'] = joined['temp'].copy()\n",
    "joined.drop(['temp'], axis=1, inplace=True)\n",
    "#joined['label'], joined['station_id'] = joined['station_id'], joined['label']\n",
    "joined['time_to_under'] = None\n",
    "joined.drop(['entrance_id','line_id'],axis=1, inplace=True)\n",
    "joined = joined.merge(mapper_s_el)\n",
    "\n",
    "#df = df.sample(10000)\n",
    "print(joined.shape)\n",
    "\n",
    "df = pd.concat([df, joined])\n",
    "df = df.sort_values(by='ticket_id')\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['ticket_type_nm','station_id','line_id','pass_hour','pass_weekday','pass_minute',\n",
    "           'schoolclidren','student','days_num','only_one','entrance_id']\n",
    "\n",
    "\n",
    "def handle(df):\n",
    "    \n",
    "\n",
    "    df['pass_dttm'] = pd.to_datetime(df['pass_dttm'])\n",
    "    df['pass_hour'] = df.pass_dttm.dt.hour\n",
    "    df['pass_weekday'] = df.pass_dttm.dt.weekday\n",
    "    df['pass_minute'] = df.pass_dttm.dt.minute\n",
    "    \n",
    "    df['schoolclidren'] = df.ticket_type_nm.apply(lambda x: int('учащ' in x))\n",
    "    df['student'] =  df.ticket_type_nm.apply(lambda x: int('студен' in x or 'аспир' in x))\n",
    "    df['days_num'] = df.ticket_type_nm.apply(\n",
    "        lambda x: 90 if '90 дней' in x else (30 if '30 д' in x else -1)\n",
    "    )\n",
    "    df['only_one'] = df.ticket_type_nm.apply(lambda x: int('един' in x)).astype(np.int32)\n",
    "    \n",
    "\n",
    "    cats_features = []\n",
    "\n",
    "    l = ['ticket_type_nm','line_id','station_id','entrance_id']\n",
    "    for c in features:\n",
    "        col_type = df[c].dtype\n",
    "        if col_type == 'object' or col_type.name == 'category' or c in l:\n",
    "            df[c] = df[c].astype('category')\n",
    "            cats_features.append(c)\n",
    "\n",
    "            \n",
    "    return df, cats_features\n",
    "\n",
    "def encode(df, col, item2id=None):\n",
    "    \n",
    "    if item2id is None:\n",
    "    \n",
    "        item2id = {v:k for k, v in enumerate(df[col].unique())}\n",
    "        id2item = {v:k for k, v in item2id.items()}\n",
    "    \n",
    "    df[col] = df[col].apply(lambda x: item2id.get(x,0)).astype('category')\n",
    "    print(len(item2id))\n",
    "    \n",
    "    return df, item2id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, cat_features = handle(df)\n",
    "df, ticket_encoder = encode(df, 'ticket_type_nm')\n",
    "df, station_encoder = encode(df, 'station_id')\n",
    "df, line_encoder = encode(df, 'line_id')\n",
    "df, entrance_encoder = encode(df, 'entrance_id')\n",
    "df, label_encoder = encode(df, 'label')\n",
    "df['label'] = df['label'].astype(np.int32)\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df#.sample(100_000)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "from scipy import special\n",
    "\n",
    "class FocalLoss:\n",
    "    \"\"\"\n",
    "    source: https://maxhalford.github.io/blog/lightgbm-focal-loss/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma, alpha=None):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def at(self, y):\n",
    "        if self.alpha is None:\n",
    "            return np.ones_like(y)\n",
    "        return np.where(y, self.alpha, 1 - self.alpha)\n",
    "\n",
    "    def pt(self, y, p):\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        return np.where(y, p, 1 - p)\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        at = self.at(y_true)\n",
    "        pt = self.pt(y_true, y_pred)\n",
    "        return -at * (1 - pt) ** self.gamma * np.log(pt)\n",
    "    def grad(self, y_true, y_pred):\n",
    "        y = 2 * y_true - 1  # {0, 1} -> {-1, 1}\n",
    "        at = self.at(y_true)\n",
    "        pt = self.pt(y_true, y_pred)\n",
    "        g = self.gamma\n",
    "        return at * y * (1 - pt) ** g * (g * pt * np.log(pt) + pt - 1)\n",
    "\n",
    "    def hess(self, y_true, y_pred):\n",
    "        y = 2 * y_true - 1  # {0, 1} -> {-1, 1}\n",
    "        at = self.at(y_true)\n",
    "        pt = self.pt(y_true, y_pred)\n",
    "        g = self.gamma\n",
    "\n",
    "        u = at * y * (1 - pt) ** g\n",
    "        du = -at * y * g * (1 - pt) ** (g - 1)\n",
    "        v = g * pt * np.log(pt) + pt - 1\n",
    "        dv = g * np.log(pt) + g + 1\n",
    "\n",
    "        return (du * v + u * dv) * y * (pt * (1 - pt))\n",
    "\n",
    "    def init_score(self, y_true):\n",
    "        res = optimize.minimize_scalar(\n",
    "            lambda p: self(y_true, p).sum(),\n",
    "            bounds=(0, 1),\n",
    "            method='bounded'\n",
    "        )\n",
    "        p = res.x\n",
    "        log_odds = np.log(p / (1 - p))\n",
    "        return log_odds\n",
    "    def lgb_obj(self, preds, train_data):\n",
    "        y = train_data.get_label()\n",
    "        p = special.expit(preds)\n",
    "        return self.grad(y, p), self.hess(y, p)\n",
    "\n",
    "    def lgb_eval(self, preds, train_data):\n",
    "        y = train_data.get_label()\n",
    "        p = special.expit(preds)\n",
    "        is_higher_better = False\n",
    "        return 'focal_loss', self(y, p).mean(), is_higher_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from sklearn.multiclass import _ConstantPredictor\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy import special\n",
    "import lightgbm as lgbm\n",
    "\n",
    "class OneVsRestLightGBMWithCustomizedLoss:\n",
    "    \"\"\"\n",
    "    source: https://towardsdatascience.com/multi-class-classification-using-focal-loss-and-lightgbm-a6a6dec28872\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss, n_jobs=3):\n",
    "        self.loss = loss\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y, **fit_params):\n",
    "        self.label_binarizer_ = LabelBinarizer(sparse_output=True)\n",
    "        Y = self.label_binarizer_.fit_transform(y)\n",
    "        Y = Y.tocsc()\n",
    "        self.classes_ = self.label_binarizer_.classes_\n",
    "        columns = (col.toarray().ravel() for col in Y.T)\n",
    "        if 'eval_set' in fit_params:\n",
    "            # use eval_set for early stopping\n",
    "            X_val, y_val = fit_params['eval_set'][0]\n",
    "            Y_val = self.label_binarizer_.transform(y_val)\n",
    "            Y_val = Y_val.tocsc()\n",
    "            columns_val = (col.toarray().ravel() for col in Y_val.T)\n",
    "            self.results_ = Parallel(n_jobs=self.n_jobs)(delayed(self._fit_binary)\n",
    "                                                         (X, column, X_val, column_val, **fit_params) for\n",
    "                                                         i, (column, column_val) in\n",
    "                                                         enumerate(zip(columns, columns_val)))\n",
    "        else:\n",
    "            # eval set not available\n",
    "            self.results_ = Parallel(n_jobs=self.n_jobs)(delayed(self._fit_binary)\n",
    "                                                         (X, column, None, None, **fit_params) for i, column\n",
    "                                                         in enumerate(columns))\n",
    "\n",
    "        return self\n",
    "    def _fit_binary(self, X, y, X_val, y_val, **fit_params):\n",
    "        unique_y = np.unique(y)\n",
    "        init_score_value = self.loss.init_score(y)\n",
    "        if len(unique_y) == 1:\n",
    "            estimator = _ConstantPredictor().fit(X, unique_y)\n",
    "        else:\n",
    "            fit = lgbm.Dataset(X, y, init_score=np.full_like(y, init_score_value, dtype=float))\n",
    "            filtering = ['eval_set', 'early_stopping_rounds', 'verbose_eval', 'num_boost_round']\n",
    "            local_fit_params = {item:value for item, value\n",
    "                                in fit_params.items() if item!='eval_set' and item != 'es'}\n",
    "            \n",
    "            if 'num_boost_round' in fit_params:\n",
    "                num_boost_round = fit_params['num_boost_round']\n",
    "            else:\n",
    "                num_boost_round = 100\n",
    "                \n",
    "            if 'early_stopping_rounds' in fit_params:\n",
    "                early_stopping_rounds = fit_params['early_stopping_rounds']\n",
    "            else:\n",
    "                early_stopping_rounds = 10\n",
    "                \n",
    "            if 'verbose_eval'  in fit_params:\n",
    "                verbose_eval = fit_params['verbose_eval']\n",
    "            else:\n",
    "                verbose_eval = 10\n",
    "                \n",
    "            if 'eval_set' in fit_params:\n",
    "                val = lgbm.Dataset(X_val, y_val, init_score=np.full_like(y_val, init_score_value, dtype=float),\n",
    "                                  reference=fit)\n",
    "        \n",
    "                estimator = lgbm.train(params=local_fit_params,\n",
    "                                       train_set=fit,\n",
    "                                       valid_sets=(fit, val),\n",
    "                                       valid_names=('fit', 'val'),\n",
    "                                       fobj=self.loss.lgb_obj,\n",
    "                                       feval=self.loss.lgb_eval,\n",
    "                                       num_boost_round=num_boost_round,\n",
    "                                       #early_stopping_rounds=early_stopping_rounds,\n",
    "                                       #verbose_eval=verbose_eval\n",
    "                                       callbacks = [fit_params['es']]\n",
    "                                      )\n",
    "            else:\n",
    "                                   \n",
    "                estimator = lgbm.train(params=local_fit_params,\n",
    "                                       train_set=fit,\n",
    "                                       fobj=self.loss.lgb_obj,\n",
    "                                       feval=self.loss.lgb_eval,\n",
    "                                       num_boost_round=num_boost_round,\n",
    "                                       early_stopping_rounds=early_stopping_rounds,\n",
    "                                       #verbose_eval=verbose_eval\n",
    "                                       callbacks = fit_params['es']\n",
    "                                      )\n",
    "\n",
    "        return estimator, init_score_value\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        maxima = np.empty(n_samples, dtype=float)\n",
    "        maxima.fill(-np.inf)\n",
    "        argmaxima = np.zeros(n_samples, dtype=int)\n",
    "\n",
    "        for i, (e, init_score) in enumerate(self.results_):\n",
    "            margins = e.predict(X, raw_score=True)\n",
    "            prob = special.expit(margins + init_score)\n",
    "            np.maximum(maxima, prob, out=maxima)\n",
    "            argmaxima[maxima == prob] = i\n",
    "\n",
    "        return argmaxima\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        y = np.zeros((X.shape[0], len(self.results_)))\n",
    "        for i, (e, init_score) in enumerate(self.results_):\n",
    "            margins = e.predict(X, raw_score=True)\n",
    "            y[:, i] = special.expit(margins + init_score)\n",
    "        y /= np.sum(y, axis=1)[:, np.newaxis]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold # k-фолдная валидация\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from lightgbm import Dataset, train\n",
    "import time\n",
    "import lightgbm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gc \n",
    "\n",
    "random_state = 42\n",
    "n_splits = 5 # kfolds\n",
    "print('num folds', n_splits)\n",
    "clfs = []\n",
    "targets = 'label'\n",
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "\n",
    "split = None\n",
    "\n",
    "if split is not None:\n",
    "    print(1)\n",
    "    X = df[features][:split]\n",
    "    y = df[targets][:split]\n",
    "else:\n",
    "    X = df[features]\n",
    "    y = df[targets]\n",
    "    \n",
    "#print(X.shape)\n",
    "#print(y.nunique())\n",
    "\n",
    "n_estimators = [20 for _ in range(n_splits)]\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    \n",
    "    s = time.time()\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    train_dataset = Dataset(data = X_train, label = y_train, categorical_feature=cat_features)\n",
    "    eval_dataset = Dataset(data = X_test, label = y_test, categorical_feature=cat_features)\n",
    "    \n",
    "    loss = FocalLoss(alpha=0.75, gamma=2.0)\n",
    "    #model = OneVsRestLightGBMWithCustomizedLoss(loss=loss)\n",
    "    \n",
    "    lgb_params = {\n",
    "         'learning_rate' : 0.1*0.9, 'n_estimators' : n_estimators[i] +10, \n",
    "        \n",
    "        'objective': 'multiclass', 'num_class' : df['label'].nunique(),\n",
    "        'uniform_drop' : True,  'boosting': 'gbdt',   # goss \n",
    "        'lambda_l2' : 1/100, 'feature_fraction': 0.65, # 0.55\n",
    "        'bagging_freq': 50, 'min_split_gain': 1/1000,\n",
    "        'max_bin' : 260,\n",
    "        'random_seed' : 42, 'drop_seed' : 7575,\n",
    "        'verbose': -1,\n",
    "        'nthreads':1\n",
    "    }\n",
    "\n",
    "    \n",
    "    fit_params = {'eval_set': [(X_test, y_test)],\n",
    "                  'num_boost_round': 200,\n",
    "                  #'early_stopping_rounds': 30,\n",
    "                   \n",
    "                  'verbosity': -1,\n",
    "                  #'verbose_eval': 10,\n",
    "                  'nthreads': 1,\n",
    "                 }\n",
    "\n",
    "    loss = FocalLoss(alpha=0.75, gamma=2.0)\n",
    "    model = OneVsRestLightGBMWithCustomizedLoss(loss=loss, n_jobs=10)\n",
    "\n",
    "    model.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "\n",
    "    clfs.append(model)\n",
    "    f = time.time()\n",
    "    print(i,'split done')\n",
    "    print('final time in hours: ', (f - s)/3600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold # k-фолдная валидация\n",
    "from lightgbm import LGBMClassifier ,LGBMRegressor\n",
    "from lightgbm import Dataset, train\n",
    "import optuna.integration.lightgbm as optuna_lgbm\n",
    "\n",
    "import optuna\n",
    "\n",
    "import gc \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gc \n",
    "\n",
    "random_state = 42\n",
    "n_splits = 10 # kfolds\n",
    "print('num folds', n_splits)\n",
    "regressors = []\n",
    "targets = 'time_to_under'\n",
    "\n",
    "n_estimators = [100, 300, 400, 300, 400, 300, 300, 300, 400, 500] # best!!\n",
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "X = df[features]#[:30000]#.drop(targets, axis=1, errors='ignore')#.copy()\n",
    "y = df[targets]#[:30000]\n",
    "\n",
    "\n",
    "X = X[y.notna()]\n",
    "y = y[y.notna()].astype(float)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    train_dataset = Dataset(data = X_train, label = y_train, categorical_feature=cat_features)\n",
    "    eval_dataset = Dataset(data = X_test, label = y_test, categorical_feature=cat_features)\n",
    "\n",
    "    print(f'fold {i}', X_train.shape, y_train.shape)\n",
    "\n",
    "    lgb_params = {\n",
    "        # 'learning_rate' : 0.1*0.9,\n",
    "        'n_estimators' : n_estimators[i] + 1000, \n",
    "        'objective': 'l2', #'num_class' : df['label'].nunique(), \n",
    "        'metric': 'l2',\n",
    "        'nthreads': 1,\n",
    "\n",
    "       # 'uniform_drop' : True, \n",
    "        'boosting': 'gbdt',   # goss \n",
    "        'lambda_l2' : 3/10000, 'feature_fraction': 0.97, # 0.55\n",
    "        'bagging_fraction': 0.8, 'min_child_samples': 56,\n",
    "        'bagging_freq': 1,\n",
    "\n",
    "        'random_seed' : 42, 'drop_seed' : 7575,\n",
    "        'verbose': -1,\n",
    "        'lambda_l1': 0.51,\n",
    "        'num_leaves': 517,\n",
    "    }\n",
    "    \n",
    "\n",
    "\n",
    "    r = lgbm.train(params = lgb_params, train_set = train_dataset,\n",
    "                   verbose_eval = 100, valid_sets=eval_dataset,\n",
    "                   callbacks=[lightgbm.early_stopping(stopping_rounds=50,verbose=0)]\n",
    "                           )\n",
    "    regressors.append(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_dataset_test (1).csv')\n",
    "test, _ = handle(test)\n",
    "test, ticket_encoder = encode(test, 'ticket_type_nm',ticket_encoder)\n",
    "test, station_encoder = encode(test, 'station_id',station_encoder)\n",
    "test, line_encoder = encode(test, 'line_id',line_encoder)\n",
    "test, entrance_encoder = encode(test, 'entrance_id',entrance_encoder)\n",
    "\n",
    "print(test.shape)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "label_decoder = {k:v for v, k in label_encoder.items()}\n",
    "\n",
    "labels = []\n",
    "times = []\n",
    "\n",
    "for chunk in tqdm(np.array_split(test[features], 1000)):\n",
    "    \n",
    "    \n",
    "    labels_drop = []\n",
    "    for n, clf in enumerate(clfs):\n",
    "        chunk[f'label_{n}'] = clf.predict(chunk[features])\n",
    "        labels_drop.append(f'label_{n}')\n",
    "        \n",
    "    labels.extend(chunk.apply(\n",
    "        lambda x: Counter([label_decoder[x[f]] for f in labels_drop]).most_common()[0][0], axis=1).values)\n",
    "    \n",
    "    regr_drop = []\n",
    "    for n, reg in enumerate(regressors):\n",
    "        chunk[f'ttu_{n}'] = reg.predict(chunk[features])\n",
    "        regr_drop.append(f'ttu_{n}')\n",
    "        \n",
    "    times.extend(chunk.apply(\n",
    "        lambda x: np.mean([x[f] for f in regr_drop]), axis=1).values)\n",
    "    \n",
    "\n",
    "    #labels.extend(model1.predict(chunk[features]).tolist())\n",
    "   # times.extend(model2.predict(chunk[features]).tolist())\n",
    "\n",
    "test['label'] = labels\n",
    "test['time_to_under'] = times\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['id','time_to_under','label']].to_csv('augmentation.csv', index=None, sep=',')\n",
    "test['label'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
